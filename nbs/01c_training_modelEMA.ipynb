{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model EMA (Exponential Moving Average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a model, it is often beneficial to maintain moving averages of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values.\n",
    "\n",
    "> NOTE: A smoothed version of the weights is necessary for some training schemes to perform well. Example Google's hyper-params for training MNASNet, MobileNet-V3, EfficientNet, etc that use RMSprop with a short 2.4-3 epoch decay period and slow LR decay rate of .96-.99 requires EMA smoothing of weights to match results.\n",
    "\n",
    "\n",
    "`timm` supports EMA similar to [tensorflow](https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage).\n",
    "\n",
    "To train models with EMA simply add the `--model-ema` flag and `--model-ema-decay` flag with a value to define the decay rate for EMA. \n",
    "\n",
    "To keep EMA from using GPU resources, set device='cpu'. This will save a bit of memory but disable validation of the EMA weights. Validation will have to be done manually in a separate process, or after the training stops converging.\n",
    "\n",
    "> NOTE: This class is sensitive where it is initialized in the sequence of model init, GPU assignment and distributed training wrappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without EMA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "python train.py ../imagenette2-320 --model resnet34\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with EMA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "python train.py ../imagenette2-320 --model resnet34 --model-ema --model-ema-decay 0.99\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above training script means that when updating the model weights, we keep 99.99% of the previous model weights and only update 0.01% of the new weights at each iteration. \n",
    "\n",
    "```python\"\n",
    "model_weights = decay * model_weights + (1 - decay) * new_model_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internals of Model EMA inside `timm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside `timm`, when we pass `--model-ema` flag then `timm` wraps the model class inside `ModelEmaV2` class which looks like:\n",
    "\n",
    "```python \n",
    "class ModelEmaV2(nn.Module):\n",
    "    def __init__(self, model, decay=0.9999, device=None):\n",
    "        super(ModelEmaV2, self).__init__()\n",
    "        # make a copy of the model for accumulating moving average of weights\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device  # perform ema on different device from model if set\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                if self.device is not None:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we initialize the `ModeEmaV2` by passing in an existing `model` and a decay rate, in this case `decay=0.9999`. \n",
    "\n",
    "This looks something like `model_ema = ModelEmaV2(model)`. Here, `model` could be any existing model as long as it's created using the `timm.create_model` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, during training especially inside the `train_one_epoch`, we call the `update` method of `model_ema` like so: \n",
    "\n",
    "```python\n",
    "if model_ema is not None:\n",
    "    model_ema.update(model)\n",
    "```\n",
    "\n",
    "All parameter updates based on `loss` occur for `model`. When we call `optimizer.step()`, then the `model` weights get updated and not the `model_ema`'s weights. \n",
    "\n",
    "Therefore, when we call the `model_ema.update` method, as can be seen, this calls the `_update` method with `update_fn = lambda e, m: self.decay * e + (1. - self.decay) * m)`. \n",
    "\n",
    "> NOTE: Basically, here, `e` refers to `model_ema` and `m` refers to the `model` whose weights get updated during training.  The `update_fn` specifies that we keep `self.decay` times the `model_ema` and `1-self.decay` times the `model`. \n",
    "\n",
    "Thus when we call the `_update` function it goes through each of the parameters inside `model` and `model_ema` and updates the state for `model_ema` to keep 99.99% of the existing state and 0.01% of the new state. \n",
    "\n",
    "> NOTE: Note that `model` and `model_ema` have the same keys inside the `state_dict`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
