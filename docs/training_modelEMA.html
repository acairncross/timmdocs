---

title: Model EMA (Exponential Moving Average)


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/01c_training_modelEMA.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01c_training_modelEMA.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When training a model, it is often beneficial to maintain moving averages of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values.
{% include note.html content='A smoothed version of the weights is necessary for some training schemes to perform well. Example Google&#8217;s hyper-params for training MNASNet, MobileNet-V3, EfficientNet, etc that use RMSprop with a short 2.4-3 epoch decay period and slow LR decay rate of .96-.99 requires EMA smoothing of weights to match results.' %}</p>
<p><code>timm</code> supports EMA similar to <a href="https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage">tensorflow</a>.</p>
<p>To train models with EMA simply add the <code>--model-ema</code> flag and <code>--model-ema-decay</code> flag with a value to define the decay rate for EMA.</p>
<p>To keep EMA from using GPU resources, set device='cpu'. This will save a bit of memory but disable validation of the EMA weights. Validation will have to be done manually in a separate process, or after the training stops converging.
{% include note.html content='This class is sensitive where it is initialized in the sequence of model init, GPU assignment and distributed training wrappers.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-without-EMA">Training without EMA<a class="anchor-link" href="#Training-without-EMA"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">model</span> <span class="n">resnet34</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-with-EMA">Training with EMA<a class="anchor-link" href="#Training-with-EMA"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">model</span> <span class="n">resnet34</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">ema</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">ema</span><span class="o">-</span><span class="n">decay</span> <span class="mf">0.99</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above training script means that when updating the model weights, we keep 99.99% of the previous model weights and only update 0.01% of the new weights at each iteration.</p>

<pre><code>python"
model_weights = decay * model_weights + (1 - decay) * new_model_weights</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Internals-of-Model-EMA-inside-timm">Internals of Model EMA inside <code>timm</code><a class="anchor-link" href="#Internals-of-Model-EMA-inside-timm"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Inside <code>timm</code>, when we pass <code>--model-ema</code> flag then <code>timm</code> wraps the model class inside <code>ModelEmaV2</code> class which looks like:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ModelEmaV2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.9999</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ModelEmaV2</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># make a copy of the model for accumulating moving average of weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay</span> <span class="o">=</span> <span class="n">decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>  <span class="c1"># perform ema on different device from model if set</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">update_fn</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">ema_v</span><span class="p">,</span> <span class="n">model_v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">model_v</span> <span class="o">=</span> <span class="n">model_v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">ema_v</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">update_fn</span><span class="p">(</span><span class="n">ema_v</span><span class="p">,</span> <span class="n">model_v</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">update_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">e</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay</span> <span class="o">*</span> <span class="n">e</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">update_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">e</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="n">m</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Basically, we initialize the <code>ModeEmaV2</code> by passing in an existing <code>model</code> and a decay rate, in this case <code>decay=0.9999</code>.</p>
<p>This looks something like <code>model_ema = ModelEmaV2(model)</code>. Here, <code>model</code> could be any existing model as long as it's created using the <code>timm.create_model</code> function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, during training especially inside the <code>train_one_epoch</code>, we call the <code>update</code> method of <code>model_ema</code> like so:</p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">model_ema</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">model_ema</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<p>All parameter updates based on <code>loss</code> occur for <code>model</code>. When we call <code>optimizer.step()</code>, then the <code>model</code> weights get updated and not the <code>model_ema</code>'s weights.</p>
<p>Therefore, when we call the <code>model_ema.update</code> method, as can be seen, this calls the <code>_update</code> method with <code>update_fn = lambda e, m: self.decay * e + (1. - self.decay) * m)</code>. 
{% include note.html content='Basically, here, <code>e</code> refers to <code>model_ema</code> and <code>m</code> refers to the <code>model</code> whose weights get updated during training.  The <code>update_fn</code> specifies that we keep <code>self.decay</code> times the <code>model_ema</code> and <code>1-self.decay</code> times the <code>model</code>. ' %}
Thus when we call the <code>_update</code> function it goes through each of the parameters inside <code>model</code> and <code>model_ema</code> and updates the state for <code>model_ema</code> to keep 99.99% of the existing state and 0.01% of the new state. 
{% include note.html content='Note that <code>model</code> and <code>model_ema</code> have the same keys inside the <code>state_dict</code>.' %}</p>

</div>
</div>
</div>
</div>
 

